{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "alphabetical_list = 'J'\n",
    "\n",
    "def get_legislation_links(soup):\n",
    "    legislation_links = []\n",
    "    \n",
    "    # Find the <ul> container containing the legislation links\n",
    "    ul_container = soup.find('ul')\n",
    "    \n",
    "    # Get all the <li> tags within the <ul> container\n",
    "    li_tags = ul_container.find_all('li')\n",
    "    \n",
    "    # Iterate through the <li> tags and extract the links\n",
    "    for li in li_tags:\n",
    "        a_tag = li.find('a')\n",
    "        if a_tag:\n",
    "            legislation_links.append(a_tag['href'])\n",
    "    \n",
    "    return legislation_links\n",
    "\n",
    "def get_section_links(soup):\n",
    "    links = []\n",
    "    for link in soup.find_all('a'):\n",
    "        href = link.get('href')\n",
    "        if href and href.startswith('s'):\n",
    "            links.append(href)\n",
    "    return links\n",
    "\n",
    "# Define the function to extract title and subsection number\n",
    "def extract_title_and_subsection(html_text):\n",
    "    soup = BeautifulSoup(html_text, 'html.parser')\n",
    "    pre_container = soup.find('pre')\n",
    "\n",
    "    subsection_elements = pre_container.find_all(lambda tag: tag.name == 'a' and tag.has_attr('href') and not tag.has_attr('name'))\n",
    "    subsection_numbers = [a.text.strip() for a in subsection_elements]\n",
    "    \n",
    "    subsection_names = []\n",
    "    for a in subsection_elements:\n",
    "        next_sibling = a.next_sibling\n",
    "        if isinstance(next_sibling, str):\n",
    "            subsection_names.append(next_sibling.strip())\n",
    "\n",
    "    act_title = soup.find('h3').text.strip()\n",
    "\n",
    "    return subsection_numbers, subsection_names, act_title\n",
    "\n",
    "\n",
    "\n",
    "data = {'Act': [],'Section_number':[], 'Section_title': [] , 'Section_text': []}\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36'\n",
    "}\n",
    "\n",
    "for letter in alphabetical_list:\n",
    "    toc_url = f'http://classic.austlii.edu.au/au/legis/act/consol_act/toc-{letter}.html'\n",
    "    response = requests.get(toc_url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    legislation_links = get_legislation_links(soup)\n",
    "    print(f\"Letter {letter} - Legislation links: {legislation_links}\")\n",
    "\n",
    "    for leg_link in legislation_links:\n",
    "        leg_url = 'http://classic.austlii.edu.au/au/legis/act/consol_act/' + leg_link + '/'\n",
    "        print(leg_link)\n",
    "        response = requests.get(leg_url, headers=headers)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        act_title = soup.find('title').text\n",
    "\n",
    "        section_links = get_section_links(soup)\n",
    "        print(f\"Act {act_title} - Section links: {section_links}\")\n",
    "        \n",
    "      \n",
    "        \n",
    "        \n",
    "        for sec_link in section_links:\n",
    "            sec_url = base_url + leg_link + '/' + sec_link\n",
    "            response = requests.get(sec_url, headers=headers)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            cleaned_text = []\n",
    "            act = soup.find('h3').text.strip().split(' - ')[0]\n",
    "            acts = soup.find('h3').text\n",
    "            section_number = acts.split()[-1]\n",
    "            section_title = soup.find('b').text\n",
    "            print(section_title)\n",
    "            section_text = soup.find_all('p')\n",
    "            for section in section_text:\n",
    "                text = section.get_text().strip()\n",
    "                text = ' '.join(text.split())  # remove extra whitespaces\n",
    "                text = re.sub('<.*?>', '', text)  # remove HTML tags\n",
    "                cleaned_text.append(text)\n",
    "            print(cleaned_text)\n",
    "            data['Section_text'].append(cleaned_text)\n",
    "            data['Act'].append(act)\n",
    "            data['Section_title'].append(section_title)\n",
    "            data['Section_number'].append(section_number)\n",
    "            print(data)\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
