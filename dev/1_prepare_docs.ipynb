{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib.parse\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.chrome.options import Options as ChromeOptions\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import requests\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base_url = 'https://www.legislation.act.gov.au'\n",
    "url = f'{base_url}/results?category=cAct&classifier=&status=Current&alpha=&query=&action=browse'\n",
    "\n",
    "\n",
    "\n",
    "def fetch_page_content(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch the page content: {url}\")\n",
    "        return None\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "def get_legislation_links(url):\n",
    "    soup = fetch_page_content(url)\n",
    "    links = soup.find_all('a')\n",
    "    legislation_links = []\n",
    "\n",
    "    for link in links:\n",
    "        href = link.get('href')\n",
    "        if href and href.startswith('/a/') and '#' not in href:\n",
    "            legislation_links.append(f'{base_url}{href}')\n",
    "\n",
    "    return legislation_links\n",
    "\n",
    "def get_html_version_link(legislation_link):\n",
    "    legislation_id = legislation_link.split('/')[-2]\n",
    "    print(f\"{base_url}/View/a/{legislation_id}/current/html/{legislation_id}.html\")\n",
    "    return f\"{base_url}/View/a/{legislation_id}/current/html/{legislation_id}.html\"\n",
    "\n",
    "def scrape_legislation_sections(html_link):\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    driver.get(html_link)\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    wait.until(EC.visibility_of_element_located((By.ID, \"progressBar\")))\n",
    "\n",
    "    legislation_soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    print(legislation_soup.prettify())\n",
    "\n",
    "    legislation_title = legislation_soup.find('title').text.strip()\n",
    "    legislation_id = html_link.split('/')[-3]\n",
    "\n",
    "    section_data = []\n",
    "\n",
    "    for section_title in legislation_soup.find_all('p', {'class': 'AH5Sec'}):\n",
    "        section_data.append(process_section_title(section_title, legislation_id, legislation_title))\n",
    "\n",
    "    for definition in legislation_soup.find_all('p', {'class': 'aDef'}):\n",
    "        section_data.append(process_text_element(definition, legislation_id, legislation_title, 'Definition'))\n",
    "\n",
    "    for defpara in legislation_soup.find_all('p', {'class': 'aDefpara'}):\n",
    "        section_data.append(process_text_element(defpara, legislation_id, legislation_title, 'DefPara'))\n",
    "\n",
    "    for note in legislation_soup.find_all('p', {'class': 'aNote'}):\n",
    "        section_data.append(process_text_element(note, legislation_id, legislation_title, 'Note'))\n",
    "\n",
    "    for note_bullet in legislation_soup.find_all('p', {'class': 'aNoteBullet'}):\n",
    "        section_data.append(process_text_element(note_bullet, legislation_id, legislation_title, 'NoteBullet'))\n",
    "\n",
    "    for main in legislation_soup.find_all('p', {'class': 'Amain'}):\n",
    "        section_data.append(process_text_element(main, legislation_id, legislation_title, 'Main'))\n",
    "\n",
    "    for para in legislation_soup.find_all('p', {'class': 'Apara'}):\n",
    "        section_data.append(process_text_element(para, legislation_id, legislation_title, 'Para'))\n",
    "\n",
    "    for subpara in legislation_soup.find_all('p', {'class': 'Asubpara'}):\n",
    "        section_data.append(process_text_element(subpara, legislation_id, legislation_title, 'SubPara'))\n",
    "\n",
    "    return section_data\n",
    "\n",
    "def process_section_title(section_title, legislation_id, legislation_title):\n",
    "    return {\n",
    "        'LegislationId': legislation_id,\n",
    "        'LegislationTitle': legislation_title,\n",
    "        'Type': 'SectionTitle',\n",
    "        'Text': section_title.get_text(strip=True).replace(section_title.find('span', {'class': 'CharSectNo'}).text, '', 1)\n",
    "    }\n",
    "\n",
    "def process_text_element(element, legislation_id, legislation_title, element_type):\n",
    "    return {\n",
    "        'LegislationId': legislation_id,\n",
    "        'LegislationTitle': legislation_title,\n",
    "        'Type': element_type,\n",
    "        'Text': element.get_text(strip=True)\n",
    "    }\n",
    "\n",
    "# Get the legislation links\n",
    "legislation_links = get_legislation_links(url)\n",
    "\n",
    "if not legislation_links:\n",
    "    print(\"No legislation links were found.\")\n",
    "else:\n",
    "    data = []\n",
    "\n",
    "    for legislation_link in legislation_links:\n",
    "        print(f\"Processing legislation link: {legislation_link}\")\n",
    "\n",
    "        html_link = get_html_version_link(legislation_link)\n",
    "        if not html_link:\n",
    "            print(f\"No HTML version link found for {legislation_link}.\")\n",
    "            continue\n",
    "\n",
    "        scraped_sections = scrape_legislation_sections(html_link)\n",
    "        print(html_link)\n",
    "        if not scraped_sections:\n",
    "            print(f\"No sections were extracted for {legislation_link}.\")\n",
    "        else:\n",
    "            print(f\"Extracted {len(scraped_sections)} sections for {legislation_link}.\")\n",
    "            data.extend(scraped_sections)\n",
    "\n",
    "    if data:\n",
    "        # Create a DataFrame and store the data\n",
    "        df = pd.DataFrame(data, columns=['LegislationId', 'LegislationTitle', 'Type', 'Text'])\n",
    "\n",
    "        # Print the DataFrame\n",
    "        print(df)\n",
    "    else:\n",
    "        print(\"No data was extracted.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
